# -*- coding: utf-8 -*-
"""Transcribe a Meeting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tzMex2NS6S4Qyvz2YS3X6qFcw911ilMa

# Speaker Diarization
"""

!pip install --upgrade pyannote.audio

!pip install -U datasets

from pyannote.audio import Pipeline

from huggingface_hub import notebook_login

notebook_login()

from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization@2.1",
                                    use_auth_token="your_token")

from datasets import load_dataset

concatenated_librispeech = load_dataset(
    "sanchit-gandhi/concatenated_librispeech", split="train", streaming=True
)
sample = next(iter(concatenated_librispeech))

from IPython.display import Audio

Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])

audio_array = sample["audio"]["array"]
sampling_rate = sample["audio"]["sampling_rate"]

import soundfile as sf

sf.write("sample.wav", audio_array, sampling_rate)

diarization = pipeline("sample.wav")

diarization

for turn, _, speaker in diarization.itertracks(yield_label=True):
    print(f"Speaker {speaker} from {turn.start:.1f}s to {turn.end:.1f}s")

"""# for general"""

import torch

input_tensor = torch.from_numpy(sample["audio"]["array"])[None,:].float()

input_tensor

outputs = pipeline(
    {"waveform": input_tensor, "sample_rate": sample["audio"]["sampling_rate"]}
)

print(outputs.to_rttm())

"""# Speech transcription"""

from transformers import pipeline

asr_pipeline = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-base",
)

asr_pipeline(sample["audio"].copy(), generate_kwargs={"max_new_tokens": 256}, return_timestamps=True)

"""# Speechbox"""

pip install git+https://github.com/huggingface/speechbox

from speechbox import ASRDiarizationPipeline

pipeline_sp = ASRDiarizationPipeline(asr_pipeline=asr_pipeline, diarization_pipeline=pipeline)

pipeline_sp(sample["audio"].copy())

def tuple_to_string(start_end_tuple, ndigits=1):
    return str((round(start_end_tuple[0], ndigits), round(start_end_tuple[1], ndigits)))

def format_as_transcription(raw_segments):
    return "\n\n".join(
        [
            chunk["speaker"] + " " + tuple_to_string(chunk["timestamp"]) + chunk["text"]
            for chunk in raw_segments
        ]
    )

outputs = pipeline_sp(sample["audio"].copy())

format_as_transcription(outputs)

def format_as_transcription(raw_segments):
  for chunk in raw_segments:
    print (f"chunk:{chunk}")

format_as_transcription(outputs)

